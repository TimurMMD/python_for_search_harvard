{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f88cfb49",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "### In this project was used dataset of tri-axial smartphone accelerometer data. The goal is to obtain the classification problem of identifying the exercise type according to accelerometer data. The project steps are following:\n",
    "\n",
    "    1. Inctantiate libraries of necessary tools\n",
    "    2. Upload the data from csv files\n",
    "    3. Data preprocessing to prepare it for the ML algorithm\n",
    "    4. Training the model\n",
    "    5. Make a prediction by using the trained model\n",
    "    6. Save the result as a new csv file\n",
    "\n",
    "***\n",
    "\n",
    "# Methods\n",
    "\n",
    "### The datasets were checked for containing any missing value, the 'timestamp' and 'UTC time' columns format was changed to datetime as it is better for Python using. After that, the 'UTC time' column was cut in terms of unnecessity (for training were used only 'x', 'y' and 'z' accelerometer data). Formatted dataset were merged in order to keep only data that has a label for a training purpose. As the last step of data preprocessing dataset were splitted for training, testing and X, y consequently.\n",
    "\n",
    "***\n",
    "# Results\n",
    "\n",
    "### The modeling results has shown low-level of accuracy ~ (<50%). Thus, the expanding the size of input data could be suggested. As the result, the model will operate with more training data and would be able to perform better. Also, the cross-validation method may vary the accuracy into higher numbers. The best running time of implementing one classification model is around 0.24 seconds.\n",
    "\n",
    "***\n",
    "# Conclusion\n",
    "\n",
    "### This project is a great chance to implement ML skills, although datasets are quite small and there is no space of applying EDA knowledge. The results are not good enough, however it's understandable how to provide better ML algorithms\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ecc45a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantianting the libraries. All Classifiers models were instantiated at one place.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "38720697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.22152233123779297 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Start timeclock \n",
    "start_time = time.time()\n",
    "\n",
    "# Read all csv files that we need for the project\n",
    "train_data = pd.read_csv('D:\\\\Work_folder\\\\Harvard_1\\\\train_time_series.csv')\n",
    "train_labels = pd.read_csv(\"D:\\\\Work_folder\\\\Harvard_1\\\\train_labels.csv\")\n",
    "test_data = pd.read_csv('D:\\\\Work_folder\\\\Harvard_1\\\\test_time_series.csv')\n",
    "test_labels = pd.read_csv(\"D:\\\\Work_folder\\\\Harvard_1\\\\test_labels.csv\")\n",
    "\n",
    "# Cut the \"UTC time\" column\n",
    "train_data = train_data[['timestamp', 'x', 'y', 'z']]\n",
    "test_data = test_data[['timestamp','x', 'y', 'z']]\n",
    "train_labels = train_labels[['timestamp', 'label']]\n",
    "test_labels = test_labels[['timestamp', 'label']]\n",
    "\n",
    "# Change the datatype of 'timestamp' column \n",
    "train_data['timestamp'] = pd.to_datetime(train_data['timestamp'])\n",
    "test_data['timestamp'] = pd.to_datetime(test_data['timestamp'])\n",
    "train_labels['timestamp'] = pd.to_datetime(train_labels['timestamp'])\n",
    "test_labels['timestamp'] = pd.to_datetime(test_labels['timestamp'])\n",
    "\n",
    "# Merging the dataset to keep only the rows with labeled data. Use 'timestamp' column as an index to merge \n",
    "merged_train_data = train_data.merge(train_labels, how='inner', on='timestamp')\n",
    "merged_test_data = test_data.merge(test_labels, how='inner', on='timestamp')\n",
    "\n",
    "# Create data for training and testing\n",
    "X_train = merged_train_data[['x', 'y', 'z']]\n",
    "y_train = merged_train_data['label']\n",
    "X_test = merged_test_data[['x', 'y', 'z']]\n",
    "\n",
    "################# RANDOM FOREST CLASSIFIER ######################\n",
    "\n",
    "# creating a RF classifier\n",
    "clf = RandomForestClassifier(n_estimators = 100)  \n",
    " \n",
    "# Training the model on the training dataset\n",
    "clf.fit(X_train, y_train)\n",
    " \n",
    "# performing predictions on the test dataset\n",
    "y_pred_rf = clf.predict(X_test)\n",
    "\n",
    "##################################################################\n",
    "\n",
    "################# GRADIENT BOOSTING CLASSIFIER ###################\n",
    "\n",
    "# creating a GB classifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0)  \n",
    " \n",
    "# Training the model on the training dataset\n",
    "clf.fit(X_train, y_train)\n",
    " \n",
    "# performing predictions on the test dataset\n",
    "y_pred_gb = clf.predict(X_test)\n",
    "\n",
    "##################################################################\n",
    "\n",
    "################# ADA BOOST CLASSIFIER ###########################\n",
    "\n",
    "# creating a AB classifier\n",
    "clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\", random_state=0) \n",
    " \n",
    "# Training the model on the training dataset\n",
    "clf.fit(X_train, y_train)\n",
    " \n",
    "# performing predictions on the test dataset\n",
    "y_pred_ab = clf.predict(X_test)\n",
    "\n",
    "##################################################################\n",
    "\n",
    "################# LOGISTIC REGRESSION ############################\n",
    "\n",
    "# creating a LR classifier\n",
    "clf = LogisticRegression(random_state=0)  \n",
    " \n",
    "# Training the model on the training dataset\n",
    "clf.fit(X_train, y_train)\n",
    " \n",
    "# performing predictions on the test dataset\n",
    "y_pred_lr = clf.predict(X_test)\n",
    "\n",
    "##################################################################\n",
    "\n",
    "################# SUPPORT VECTOR CLASSIFIER ######################\n",
    "\n",
    "# creating a SV classifier\n",
    "clf = SVC(gamma='auto')  \n",
    " \n",
    "# Training the model on the training dataset\n",
    "clf.fit(X_train, y_train)\n",
    " \n",
    "# performing predictions on the test dataset\n",
    "y_pred_sv = clf.predict(X_test)\n",
    "\n",
    "##################################################################\n",
    "\n",
    "# Convert array into dataframe. I decided to save Random Forest Classifier results, as they showed the best results.\n",
    "DF = pd.DataFrame(y_pred_rf)\n",
    "\n",
    "test_label = test_labels.join(DF, how='outer')\n",
    "test_label.drop('label', axis=1, inplace=True)\n",
    "test_label = test_label.rename(columns={0: \"label\"})\n",
    "\n",
    "# save the dataframe as a csv file \n",
    "test_label.to_csv(\"D:\\\\Work_folder\\\\Harvard_1\\\\predictions_rf.csv\")\n",
    "#The time is provided for only RF classifier, as it has shown the best performance. If execute this code, it obviously would be much longer \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
